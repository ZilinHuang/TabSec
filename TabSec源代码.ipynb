{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4RojhzF26z"
      },
      "source": [
        "# 挂载文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dimyrxaQ7JyA",
        "outputId": "67c4220d-552d-46d9-8b2c-e9838ef57a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MlTHCUs3xYD"
      },
      "source": [
        "# KDD-UEBA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNKitHkEFE1l"
      },
      "source": [
        "## 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKTI6HDP32so",
        "outputId": "0db0b527-fff0-464c-b616-9a5cd9944c50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([0., 1., 2., 3., 4., 5., 6.]), array([ 97277, 391458,   1717,  23681,  78058, 226326,  52052]))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "kddcup99 = pd.read_csv('/content/drive/MyDrive/kddcup99(raw).csv',encoding='gbk')\n",
        "kddcup99.columns = kddcup99.columns.str.strip()\n",
        "kddcup99.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "kddcup99.dropna(inplace=True)\n",
        "\n",
        "def map_label_to_category(label):\n",
        "    probe_attacks = ['ipsweep', 'nmap', 'portsweep']\n",
        "    dos_attacks = ['smurf', 'neptune', 'back','teardrop','pod','land']\n",
        "    u2r_attacks = ['buffer_overflow','rootkit','loadmodule','perl']\n",
        "    r2l_attacks = ['ftp_write', 'imap', 'multihop','phf', 'spy', 'warezclient','warezmaster','guess_passwd']\n",
        "\n",
        "    if label in probe_attacks:\n",
        "        return 'Probe'\n",
        "    elif label in dos_attacks:\n",
        "        return 'DoS'\n",
        "    elif label in u2r_attacks:\n",
        "        return 'U2R'\n",
        "    elif label in r2l_attacks:\n",
        "        return 'R2L'\n",
        "    elif label == 'normal':\n",
        "        return 'BENIGN'\n",
        "\n",
        "\n",
        "kddcup99['label'] = kddcup99['label'].apply(map_label_to_category)\n",
        "# 检查新的类别分布\n",
        "category_counts = kddcup99['label'].value_counts()\n",
        "kddcup99['label'].value_counts()\n",
        "\n",
        "values_list = ['BENIGN','Probe', 'DoS', 'U2R','R2L']\n",
        "filtered_df = kddcup99[kddcup99['label'].isin(values_list)]\n",
        "\n",
        "\n",
        "pd_train_v1 = pd.read_csv('/content/drive/MyDrive/CVUEBA.csv',encoding='gbk')\n",
        "pd_train_v1 = pd_train_v1.rename(columns={'attack': 'label'})\n",
        "\n",
        "\n",
        "def get_label(Label):\n",
        "    if Label == 0:\n",
        "        return 'Normal'\n",
        "    else:\n",
        "        return 'Malicious'\n",
        "\n",
        "pd_train_v1['label'] = pd_train_v1['label'].apply(get_label)\n",
        "\n",
        "all_columns = set(filtered_df.columns) | set(pd_train_v1.columns)\n",
        "filtered_df_v1 = filtered_df.reindex(columns=all_columns)\n",
        "df_train_v2 = pd_train_v1.reindex(columns=all_columns)\n",
        "pd_final = pd.concat([filtered_df_v1, df_train_v2], ignore_index=True)\n",
        "\n",
        "nb_class = len(list(pd_final['label'].value_counts()))\n",
        "pd_mini = pd_final.sample(n=len(pd_final))\n",
        "\n",
        "object_cols = pd_mini.select_dtypes(include=['object']).columns.tolist()\n",
        "object_cols.remove('label')\n",
        "\n",
        "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value=-99))\n",
        "cat_ohe_step = ('ohe', OrdinalEncoder()) #离散数据整数化处理\n",
        "cat_steps = [cat_si_step, cat_ohe_step]\n",
        "\n",
        "cat_pipe = Pipeline(cat_steps)\n",
        "cat_cols = object_cols\n",
        "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=cat_transformers)\n",
        "\n",
        "\n",
        "for column_name in cat_cols:\n",
        "  pd_mini[column_name] = pd_mini[column_name].astype('str')\n",
        "X_kdd_transformed = ct.fit_transform(pd_mini)\n",
        "non_object_cols = pd_mini.select_dtypes(exclude=['object']).columns.tolist()\n",
        "num_cols = non_object_cols\n",
        "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
        "num_ss_step = ('ss', StandardScaler()) #均值-标准差归一化\n",
        "num_steps = [num_si_step, num_ss_step]\n",
        "num_pipe = Pipeline(num_steps)\n",
        "num_transformers = [('num', num_pipe, num_cols)]\n",
        "ct = ColumnTransformer(transformers=num_transformers)\n",
        "X_num_transformed = ct.fit_transform(pd_mini)\n",
        "\n",
        "transformers = [('cat', cat_pipe, cat_cols),\n",
        "                ('num', num_pipe, num_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=transformers)\n",
        "datax = pd_mini.drop(columns=['label'])\n",
        "x_all = ct.fit_transform(datax)\n",
        "le = LabelEncoder()\n",
        "y_all = le.fit_transform(pd_mini['label'])\n",
        "y_all = y_all.reshape(-1,1)\n",
        "data = np.concatenate((y_all.reshape(-1,1),x_all),axis=1)\n",
        "c = []\n",
        "#重采样\n",
        "for i in range(len(pd_final)):\n",
        "    if data[i,0] == 2:\n",
        "        data = np.concatenate((data,np.tile(data[i,:],(100,1))))\n",
        "    if data[i, 0] == 4:\n",
        "        data = np.concatenate((data, np.tile(data[i, :], (30, 1))))\n",
        "    if data[i, 0] == 5:\n",
        "        data = np.concatenate((data, np.tile(data[i, :], (200, 1))))\n",
        "    if data[i,0] == 6:\n",
        "        data = np.concatenate((data,np.tile(data[i,:],(1000,1))))\n",
        "print(np.unique(data[:,0], return_counts=True))\n",
        "#转化为独热编码\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_samples = len(labels)\n",
        "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
        "    for i in range(num_samples):\n",
        "        one_hot_labels[i, labels[i]] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "num_classes = 7\n",
        "one_hot_labels = one_hot_encode(data[:,0].astype(int), num_classes)\n",
        "\n",
        "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(data[:,1:], one_hot_labels,stratify=one_hot_labels, test_size=0.1, random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOUgWOn9xJiB",
        "outputId": "adff08fb-b5e4-4168-d5c3-a32c7e5e39a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "516129\n"
          ]
        }
      ],
      "source": [
        "print(len(pd_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEIiI3dQyRPi"
      },
      "source": [
        "## Tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5r9mjWL1wV7"
      },
      "outputs": [],
      "source": [
        "y_train_1 = np.argmax(y_train_1, axis=1)\n",
        "y_test_1 = np.argmax(y_test_1, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjk9044Q2CbM",
        "outputId": "6c8ecf2e-b345-45e8-ac4f-d55f71a4afd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.14707 |  0:00:32s\n",
            "epoch 1  | loss: 0.03319 |  0:01:03s\n",
            "epoch 2  | loss: 0.02864 |  0:01:33s\n",
            "epoch 3  | loss: 0.02415 |  0:02:04s\n",
            "epoch 4  | loss: 0.02483 |  0:02:35s\n",
            "epoch 5  | loss: 0.02535 |  0:03:05s\n",
            "epoch 6  | loss: 0.02545 |  0:03:36s\n",
            "epoch 7  | loss: 0.03065 |  0:04:07s\n",
            "epoch 8  | loss: 0.02434 |  0:04:37s\n",
            "epoch 9  | loss: 0.02035 |  0:05:08s\n",
            "epoch 10 | loss: 0.02167 |  0:05:39s\n",
            "epoch 11 | loss: 0.02101 |  0:06:09s\n",
            "epoch 12 | loss: 0.01908 |  0:06:40s\n",
            "epoch 13 | loss: 0.01956 |  0:07:10s\n",
            "epoch 14 | loss: 0.0179  |  0:07:41s\n",
            "epoch 15 | loss: 0.01932 |  0:08:11s\n",
            "epoch 16 | loss: 0.01718 |  0:08:42s\n",
            "epoch 17 | loss: 0.017   |  0:09:13s\n",
            "epoch 18 | loss: 0.016   |  0:09:43s\n",
            "epoch 19 | loss: 0.0186  |  0:10:14s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "# 定义TabNet模型\n",
        "tabnet_params = {\n",
        "    'n_d': 8,\n",
        "    'n_a': 8,\n",
        "    'n_steps': 3,\n",
        "    'gamma': 1.5,\n",
        "    'n_independent': 2,\n",
        "    'n_shared': 2,\n",
        "    'momentum': 0.02,\n",
        "    'lambda_sparse': 1e-3,\n",
        "    'optimizer_fn': torch.optim.Adam,\n",
        "    'optimizer_params': dict(lr=2e-2),\n",
        "    'mask_type': 'sparsemax',\n",
        "    'scheduler_params': {\"gamma\": 0.95,\n",
        "                         \"step_size\": 10},\n",
        "    'scheduler_fn': torch.optim.lr_scheduler.StepLR\n",
        "}\n",
        "\n",
        "X_train = torch.tensor(x_train_1, dtype=torch.float32)\n",
        "X_test = torch.tensor(x_test_1, dtype=torch.float32)\n",
        "\n",
        "if y_train_1.ndim == 2:\n",
        "    y_train = torch.tensor(np.argmax(y_train_1, axis=1), dtype=torch.long)\n",
        "    y_test = torch.tensor(np.argmax(y_test_1, axis=1), dtype=torch.long)\n",
        "else:\n",
        "    y_train = torch.tensor(y_train_1, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test_1, dtype=torch.long)\n",
        "\n",
        "clf = TabNetClassifier(**tabnet_params)\n",
        "\n",
        "clf.fit(\n",
        "    x_train_1,\n",
        "    y_train,\n",
        "    max_epochs=20,\n",
        "    patience=20,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjJ4938zakdn"
      },
      "source": [
        "## 分类报告"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88LfAT4YTbrT"
      },
      "outputs": [],
      "source": [
        "if hasattr(y_pred, 'detach'):\n",
        "    y_pred = y_pred.detach().cpu().numpy()\n",
        "elif hasattr(y_pred, 'numpy'):\n",
        "    y_pred = y_pred.numpy()\n",
        "\n",
        "if hasattr(y_test, 'detach'):\n",
        "    y_test = y_test.detach().cpu().numpy()\n",
        "elif hasattr(y_test, 'numpy'):\n",
        "    y_test = y_test.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxNvkxoDwo-K"
      },
      "outputs": [],
      "source": [
        "# 打印分类报告\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import recall_score, confusion_matrix, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jQCmYvwi2JA",
        "outputId": "a82d3f05-30dd-452c-97f4-49d03e5fbfb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.9949\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      BENIGN     0.9983    0.9881    0.9932      9728\n",
            "         DoS     0.9999    0.9998    0.9999     39146\n",
            "   Malicious     0.9942    1.0000    0.9971       172\n",
            "      Normal     1.0000    0.9996    0.9998      2368\n",
            "       Probe     0.9988    0.9986    0.9987      7806\n",
            "         R2L     0.9831    0.9990    0.9910     22632\n",
            "         U2R     0.9941    0.9439    0.9684      5205\n",
            "\n",
            "    accuracy                         0.9949     87057\n",
            "   macro avg     0.9955    0.9899    0.9926     87057\n",
            "weighted avg     0.9949    0.9949    0.9948     87057\n",
            "\n",
            "Accuracy: 0.9949\n",
            "Detection Rate: 0.9899\n",
            "Confusion Matrix:\n",
            "[[ 9612     5     0     0     9    95     7]\n",
            " [    5 39140     0     0     0     1     0]\n",
            " [    0     0   172     0     0     0     0]\n",
            " [    0     0     1  2367     0     0     0]\n",
            " [   11     0     0     0  7795     0     0]\n",
            " [    0     0     0     0     0 22610    22]\n",
            " [    0     0     0     0     0   292  4913]]\n",
            "False Alarm Rate: 0.0010\n",
            "False Negative Rate: 0.0101\n"
          ]
        }
      ],
      "source": [
        "# 评估模型\n",
        "y_pred = clf.predict(x_test_1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Final Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 将预测和真实标签直接映射回原始类别标签\n",
        "y_test_original = le.inverse_transform(y_test)\n",
        "y_pred_original = le.inverse_transform(y_pred)\n",
        "\n",
        "print(classification_report(y_test_original, y_pred_original, digits=4))\n",
        "\n",
        "\n",
        "# 预测\n",
        "y_pred = clf.predict(x_test_1)\n",
        "\n",
        "# 准确率\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 检测率（召回率）\n",
        "detection_rate = recall_score(y_test, y_pred, average='macro')  # 使用 'macro' 或 'weighted' 平均\n",
        "print(f'Detection Rate: {detection_rate:.4f}')\n",
        "\n",
        "# 混淆矩阵\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "false_alarm_rate = []\n",
        "false_negative_rate = []\n",
        "\n",
        "for i in range(len(conf_matrix)):\n",
        "    tp = conf_matrix[i, i]\n",
        "    fp = conf_matrix[:, i].sum() - tp\n",
        "    fn = conf_matrix[i, :].sum() - tp\n",
        "    tn = conf_matrix.sum() - (tp + fp + fn)\n",
        "\n",
        "    if fp + tn > 0:\n",
        "        false_alarm_rate.append(fp / (fp + tn))\n",
        "    if fn + tp > 0:\n",
        "        false_negative_rate.append(fn / (fn + tp))\n",
        "\n",
        "\n",
        "avg_false_alarm_rate = sum(false_alarm_rate) / len(false_alarm_rate)\n",
        "avg_false_negative_rate = sum(false_negative_rate) / len(false_negative_rate)\n",
        "\n",
        "print(f'False Alarm Rate: {avg_false_alarm_rate:.4f}')\n",
        "print(f'False Negative Rate: {avg_false_negative_rate:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8pXeRE9SAdI",
        "outputId": "81d606f5-1e5a-4f31-a92f-f1a241756ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['BENIGN' 'DoS' 'Malicious' 'Normal' 'Probe' 'R2L' 'U2R']\n"
          ]
        }
      ],
      "source": [
        "print(le.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ynOArDb6p4"
      },
      "source": [
        "# NSL-UEBA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVJ_Bo8XEBTP"
      },
      "source": [
        "## 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXy3-ReHwu58"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "NSL_KDD = pd.read_csv('/content/drive/MyDrive/NSL_KDD.csv',encoding='gbk')\n",
        "NSL_KDD.columns = NSL_KDD.columns.str.strip()\n",
        "NSL_KDD.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "NSL_KDD.dropna(inplace=True)\n",
        "\n",
        "def map_label_to_category(label):\n",
        "    probe_attacks = ['satan', 'ipsweep', 'nmap', 'portsweep', 'mscan', 'saint']\n",
        "    dos_attacks = ['neptune', 'smurf', 'back', 'teardrop', 'pod', 'land', 'apache2', 'processtable', 'mailbomb', 'snmpgetattack', 'httptunnel', 'worm', 'sqlattack', 'udpstorm']\n",
        "    u2r_attacks = ['buffer_overflow', 'loadmodule', 'rootkit', 'perl', 'xterm', 'ps']\n",
        "    r2l_attacks = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster', 'warezclient', 'spy', 'named', 'sendmail', 'snmpguess', 'xlock', 'xsnoop']\n",
        "\n",
        "\n",
        "    if label in probe_attacks:\n",
        "        return 'Probe'\n",
        "    elif label in dos_attacks:\n",
        "        return 'DoS'\n",
        "    elif label in u2r_attacks:\n",
        "        return 'U2R'\n",
        "    elif label in r2l_attacks:\n",
        "        return 'R2L'\n",
        "    elif label == 'normal':\n",
        "        return 'BENIGN'\n",
        "\n",
        "NSL_KDD['label'] = NSL_KDD['labels'].apply(map_label_to_category)\n",
        "# 检查新的类别分布\n",
        "category_counts = NSL_KDD['label'].value_counts()\n",
        "NSL_KDD['label'].value_counts()\n",
        "\n",
        "values_list = ['BENIGN','Probe', 'DoS', 'U2R','R2L']\n",
        "filtered_df = NSL_KDD[NSL_KDD['label'].isin(values_list)]\n",
        "\n",
        "NSL_KDD['label'] = NSL_KDD['labels'].apply(map_label_to_category)\n",
        "\n",
        "pd_train_v1 = pd.read_csv('/content/drive/MyDrive/CVUEBA.csv',encoding='gbk')\n",
        "pd_train_v1 = pd_train_v1.rename(columns={'attack': 'label'})\n",
        "\n",
        "\n",
        "def get_label(Label):\n",
        "    if Label == 0:\n",
        "        return 'Normal'\n",
        "    else:\n",
        "        return 'Malicious'\n",
        "\n",
        "pd_train_v1['label'] = pd_train_v1['label'].apply(get_label)\n",
        "all_columns = set(filtered_df.columns) | set(pd_train_v1.columns)\n",
        "filtered_df_v1 = filtered_df.reindex(columns=all_columns)\n",
        "df_train_v2 = pd_train_v1.reindex(columns=all_columns)\n",
        "pd_final = pd.concat([filtered_df_v1, df_train_v2], ignore_index=True)\n",
        "\n",
        "nb_class = len(list(pd_final['label'].value_counts()))\n",
        "pd_mini = pd_final.sample(n=len(pd_final))\n",
        "\n",
        "object_cols = pd_mini.select_dtypes(include=['object']).columns.tolist()\n",
        "object_cols.remove('label')\n",
        "\n",
        "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value=-99))\n",
        "cat_ohe_step = ('ohe', OrdinalEncoder()) #离散数据整数化处理\n",
        "cat_steps = [cat_si_step, cat_ohe_step]\n",
        "\n",
        "cat_pipe = Pipeline(cat_steps)\n",
        "cat_cols = object_cols\n",
        "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=cat_transformers)\n",
        "\n",
        "\n",
        "for column_name in cat_cols:\n",
        "  pd_mini[column_name] = pd_mini[column_name].astype('str')\n",
        "X_kdd_transformed = ct.fit_transform(pd_mini)\n",
        "non_object_cols = pd_mini.select_dtypes(exclude=['object']).columns.tolist()\n",
        "num_cols = non_object_cols\n",
        "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
        "num_ss_step = ('ss', StandardScaler()) #均值-标准差归一化\n",
        "num_steps = [num_si_step, num_ss_step]\n",
        "num_pipe = Pipeline(num_steps)\n",
        "num_transformers = [('num', num_pipe, num_cols)]\n",
        "ct = ColumnTransformer(transformers=num_transformers)\n",
        "X_num_transformed = ct.fit_transform(pd_mini)\n",
        "\n",
        "transformers = [('cat', cat_pipe, cat_cols),\n",
        "                ('num', num_pipe, num_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=transformers)\n",
        "datax = pd_mini.drop(columns=['label'])\n",
        "x_all = ct.fit_transform(datax)\n",
        "le = LabelEncoder()\n",
        "y_all = le.fit_transform(pd_mini['label'])\n",
        "y_all = y_all.reshape(-1,1)\n",
        "data = np.concatenate((y_all.reshape(-1,1),x_all),axis=1)\n",
        "c = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-aKkMsP6AsK",
        "outputId": "e2af7359-4c3d-4d71-fe31-ce4b3da5cc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([0., 1., 2., 3., 4., 5., 6.]), array([ 77054,  53700,   1717,  23681,  14077, 110639,   9477]))\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(pd_final)):\n",
        "    if data[i,0] == 2:\n",
        "        data = np.concatenate((data,np.tile(data[i,:],(100,1))))\n",
        "    if data[i, 0] == 5:\n",
        "        data = np.concatenate((data, np.tile(data[i, :], (30, 1))))\n",
        "    if data[i,0] == 6:\n",
        "        data = np.concatenate((data,np.tile(data[i,:],(80,1))))\n",
        "print(np.unique(data[:,0], return_counts=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Ff_60Uejdm",
        "outputId": "9c7155d1-8e04-4a7b-ea37-a4e28aa81d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172215\n"
          ]
        }
      ],
      "source": [
        "print(len(x_all))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fIy-wRWZXRp"
      },
      "outputs": [],
      "source": [
        "# 编码\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_samples = len(labels)\n",
        "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
        "    for i in range(num_samples):\n",
        "        one_hot_labels[i, labels[i]] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "num_classes = 7\n",
        "one_hot_labels = one_hot_encode(data[:,0].astype(int), num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ4Y2DFz3LhI"
      },
      "source": [
        "## 加载数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ZZ27iDh0XZ"
      },
      "outputs": [],
      "source": [
        "x_train_NSL_KDD, x_test_NSL_KDD, y_train_NSL_KDD, y_test_NSL_KDD = train_test_split(data[:,1:], one_hot_labels,stratify=one_hot_labels, test_size=0.2, random_state=12) # 固定了随机数种子导致分割的数据集是重复的"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_jCBkcXhELO"
      },
      "source": [
        "## Tabnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNfvOORPgpZm"
      },
      "outputs": [],
      "source": [
        "y_train_NSL_KDD = np.argmax(y_train_NSL_KDD, axis=1)\n",
        "y_test_NSL_KDD = np.argmax(y_test_NSL_KDD, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbTFzuwJkXGi",
        "outputId": "cb0db858-d25f-4ae5-ee61-ed5e007e7e52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.33167 |  0:00:08s\n",
            "epoch 1  | loss: 0.04716 |  0:00:17s\n",
            "epoch 2  | loss: 0.04822 |  0:00:25s\n",
            "epoch 3  | loss: 0.02737 |  0:00:34s\n",
            "epoch 4  | loss: 0.0148  |  0:00:43s\n",
            "epoch 5  | loss: 0.01388 |  0:00:51s\n",
            "epoch 6  | loss: 0.01439 |  0:01:00s\n",
            "epoch 7  | loss: 0.01129 |  0:01:09s\n",
            "epoch 8  | loss: 0.01062 |  0:01:17s\n",
            "epoch 9  | loss: 0.01088 |  0:01:26s\n",
            "epoch 10 | loss: 0.00942 |  0:01:35s\n",
            "epoch 11 | loss: 0.01086 |  0:01:43s\n",
            "epoch 12 | loss: 0.00759 |  0:01:52s\n",
            "epoch 13 | loss: 0.00818 |  0:02:01s\n",
            "epoch 14 | loss: 0.01221 |  0:02:09s\n",
            "epoch 15 | loss: 0.00795 |  0:02:18s\n",
            "epoch 16 | loss: 0.00654 |  0:02:27s\n",
            "epoch 17 | loss: 0.00693 |  0:02:36s\n",
            "epoch 18 | loss: 0.00803 |  0:02:44s\n",
            "epoch 19 | loss: 0.00741 |  0:02:53s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 定义TabNet模型\n",
        "tabnet_params = {\n",
        "    'n_d': 8,\n",
        "    'n_a': 8,\n",
        "    'n_steps': 3,\n",
        "    'gamma': 1.5,\n",
        "    'n_independent': 2,\n",
        "    'n_shared': 2,\n",
        "    'momentum': 0.02,\n",
        "    'lambda_sparse': 1e-3,\n",
        "    'optimizer_fn': torch.optim.Adam,\n",
        "    'optimizer_params': dict(lr=2e-2),\n",
        "    'mask_type': 'sparsemax',\n",
        "    'scheduler_params': {\"gamma\": 0.95,\n",
        "                         \"step_size\": 10},\n",
        "    'scheduler_fn': torch.optim.lr_scheduler.StepLR\n",
        "}\n",
        "\n",
        "X_train = torch.tensor(x_train_1, dtype=torch.float32)\n",
        "X_test = torch.tensor(x_test_1, dtype=torch.float32)\n",
        "\n",
        "if y_train_1.ndim == 2:\n",
        "    y_train = torch.tensor(np.argmax(y_train_1, axis=1), dtype=torch.long)\n",
        "    y_test = torch.tensor(np.argmax(y_test_1, axis=1), dtype=torch.long)\n",
        "else:\n",
        "    y_train = torch.tensor(y_train_1, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test_1, dtype=torch.long)\n",
        "\n",
        "clf = TabNetClassifier(**tabnet_params)\n",
        "\n",
        "clf.fit(\n",
        "    x_train_1,\n",
        "    y_train,\n",
        "    max_epochs=20,\n",
        "    patience=20,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGZE4V9hxJFR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import recall_score, confusion_matrix, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPTauN00fo3q",
        "outputId": "00cf7f00-5e4f-43b4-e749-e583490a32b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.9986\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      BENIGN     1.0000    0.9989    0.9994     15411\n",
            "         DoS     0.9983    0.9966    0.9975     10740\n",
            "   Malicious     0.9942    1.0000    0.9971       343\n",
            "      Normal     1.0000    0.9996    0.9998      4736\n",
            "       Probe     0.9989    0.9940    0.9964      2815\n",
            "         R2L     0.9981    0.9995    0.9988     22128\n",
            "         U2R     0.9911    1.0000    0.9955      1896\n",
            "\n",
            "    accuracy                         0.9986     58069\n",
            "   macro avg     0.9972    0.9984    0.9978     58069\n",
            "weighted avg     0.9986    0.9986    0.9986     58069\n",
            "\n",
            "Accuracy: 0.9986\n",
            "Detection Rate: 0.9984\n",
            "Confusion Matrix:\n",
            "[[15394     0     0     0     0     0    17]\n",
            " [    0 10704     0     0     3    33     0]\n",
            " [    0     0   343     0     0     0     0]\n",
            " [    0     0     2  4734     0     0     0]\n",
            " [    0     7     0     0  2798    10     0]\n",
            " [    0    11     0     0     0 22117     0]\n",
            " [    0     0     0     0     0     0  1896]]\n",
            "False Alarm Rate: 0.0003\n",
            "False Negative Rate: 0.0016\n"
          ]
        }
      ],
      "source": [
        "# 评估模型\n",
        "y_pred = clf.predict(x_test_NSL_KDD)\n",
        "accuracy = accuracy_score(y_test_NSL_KDD, y_pred)\n",
        "print(f'Final Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 将预测和真实标签直接映射回原始类别标签\n",
        "y_test_original = le.inverse_transform(y_test)\n",
        "y_pred_original = le.inverse_transform(y_pred)\n",
        "\n",
        "print(classification_report(y_test_original, y_pred_original, digits=4))\n",
        "\n",
        "# 预测\n",
        "y_pred = clf.predict(x_test_NSL_KDD)\n",
        "\n",
        "# 准确率\n",
        "accuracy = accuracy_score(y_test_NSL_KDD, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 检测率（召回率）\n",
        "detection_rate = recall_score(y_test_NSL_KDD, y_pred, average='macro')  # 使用 'macro' 或 'weighted' 平均\n",
        "print(f'Detection Rate: {detection_rate:.4f}')\n",
        "\n",
        "# 混淆矩阵\n",
        "conf_matrix = confusion_matrix(y_test_NSL_KDD, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "false_alarm_rate = []\n",
        "false_negative_rate = []\n",
        "\n",
        "for i in range(len(conf_matrix)):\n",
        "    tp = conf_matrix[i, i]\n",
        "    fp = conf_matrix[:, i].sum() - tp\n",
        "    fn = conf_matrix[i, :].sum() - tp\n",
        "    tn = conf_matrix.sum() - (tp + fp + fn)\n",
        "\n",
        "    if fp + tn > 0:\n",
        "        false_alarm_rate.append(fp / (fp + tn))\n",
        "    if fn + tp > 0:\n",
        "        false_negative_rate.append(fn / (fn + tp))\n",
        "\n",
        "avg_false_alarm_rate = sum(false_alarm_rate) / len(false_alarm_rate)\n",
        "avg_false_negative_rate = sum(false_negative_rate) / len(false_negative_rate)\n",
        "\n",
        "print(f'False Alarm Rate: {avg_false_alarm_rate:.4f}')\n",
        "print(f'False Negative Rate: {avg_false_negative_rate:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0p5-PqLR066"
      },
      "source": [
        "# 对比试验(NSL-UEBA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_-fotuPNQR"
      },
      "source": [
        "## 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwTiNJnLPNQS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "NSL_KDD = pd.read_csv('/content/drive/MyDrive/NSL_KDD.csv',encoding='gbk')\n",
        "NSL_KDD.columns = NSL_KDD.columns.str.strip()\n",
        "NSL_KDD.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "NSL_KDD.dropna(inplace=True)\n",
        "\n",
        "def map_label_to_category(label):\n",
        "    probe_attacks = ['satan', 'ipsweep', 'nmap', 'portsweep', 'mscan', 'saint']\n",
        "    dos_attacks = ['neptune', 'smurf', 'back', 'teardrop', 'pod', 'land', 'apache2', 'processtable', 'mailbomb', 'snmpgetattack', 'httptunnel', 'worm', 'sqlattack', 'udpstorm']\n",
        "    u2r_attacks = ['buffer_overflow', 'loadmodule', 'rootkit', 'perl', 'xterm', 'ps']\n",
        "    r2l_attacks = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster', 'warezclient', 'spy', 'named', 'sendmail', 'snmpguess', 'xlock', 'xsnoop']\n",
        "\n",
        "    if label in probe_attacks:\n",
        "        return 'Probe'\n",
        "    elif label in dos_attacks:\n",
        "        return 'DoS'\n",
        "    elif label in u2r_attacks:\n",
        "        return 'U2R'\n",
        "    elif label in r2l_attacks:\n",
        "        return 'R2L'\n",
        "    elif label == 'normal':\n",
        "        return 'BENIGN'\n",
        "\n",
        "NSL_KDD['label'] = NSL_KDD['labels'].apply(map_label_to_category)\n",
        "\n",
        "category_counts = NSL_KDD['label'].value_counts()\n",
        "NSL_KDD['label'].value_counts()\n",
        "\n",
        "values_list = ['BENIGN','Probe', 'DoS', 'U2R','R2L']\n",
        "filtered_df = NSL_KDD[NSL_KDD['label'].isin(values_list)]\n",
        "\n",
        "\n",
        "NSL_KDD['label'] = NSL_KDD['labels'].apply(map_label_to_category)\n",
        "\n",
        "pd_train_v1 = pd.read_csv('/content/drive/MyDrive/CVUEBA.csv',encoding='gbk')\n",
        "pd_train_v1 = pd_train_v1.rename(columns={'attack': 'label'})\n",
        "\n",
        "\n",
        "def get_label(Label):\n",
        "    if Label == 0:\n",
        "        return 'Normal'\n",
        "    else:\n",
        "        return 'Malicious'\n",
        "\n",
        "pd_train_v1['label'] = pd_train_v1['label'].apply(get_label)\n",
        "# pd_train_v1.drop(columns=['day'],inplace=True)\n",
        "\n",
        "all_columns = set(filtered_df.columns) | set(pd_train_v1.columns)\n",
        "filtered_df_v1 = filtered_df.reindex(columns=all_columns)\n",
        "df_train_v2 = pd_train_v1.reindex(columns=all_columns)\n",
        "pd_final = pd.concat([filtered_df_v1, df_train_v2], ignore_index=True)\n",
        "\n",
        "nb_class = len(list(pd_final['label'].value_counts()))\n",
        "pd_mini = pd_final.sample(n=len(pd_final))\n",
        "\n",
        "object_cols = pd_mini.select_dtypes(include=['object']).columns.tolist()\n",
        "object_cols.remove('label')\n",
        "\n",
        "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value=-99))\n",
        "cat_ohe_step = ('ohe', OrdinalEncoder()) #离散数据整数化处理\n",
        "cat_steps = [cat_si_step, cat_ohe_step]\n",
        "\n",
        "cat_pipe = Pipeline(cat_steps)\n",
        "cat_cols = object_cols\n",
        "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=cat_transformers)\n",
        "\n",
        "\n",
        "for column_name in cat_cols:\n",
        "  pd_mini[column_name] = pd_mini[column_name].astype('str')\n",
        "X_kdd_transformed = ct.fit_transform(pd_mini)\n",
        "non_object_cols = pd_mini.select_dtypes(exclude=['object']).columns.tolist()\n",
        "num_cols = non_object_cols\n",
        "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
        "num_ss_step = ('ss', StandardScaler()) #均值-标准差归一化\n",
        "num_steps = [num_si_step, num_ss_step]\n",
        "num_pipe = Pipeline(num_steps)\n",
        "num_transformers = [('num', num_pipe, num_cols)]\n",
        "ct = ColumnTransformer(transformers=num_transformers)\n",
        "X_num_transformed = ct.fit_transform(pd_mini)\n",
        "\n",
        "transformers = [('cat', cat_pipe, cat_cols),\n",
        "                ('num', num_pipe, num_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=transformers)\n",
        "datax = pd_mini.drop(columns=['label'])\n",
        "x_all = ct.fit_transform(datax)\n",
        "le = LabelEncoder()\n",
        "y_all = le.fit_transform(pd_mini['label'])\n",
        "y_all = y_all.reshape(-1,1)\n",
        "data = np.concatenate((y_all.reshape(-1,1),x_all),axis=1)\n",
        "c = []\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_samples = len(labels)\n",
        "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
        "    for i in range(num_samples):\n",
        "        one_hot_labels[i, labels[i]] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "num_classes = 7\n",
        "one_hot_labels = one_hot_encode(data[:,0].astype(int), num_classes)\n",
        "\n",
        "x_train_NSL_KDD, x_test_NSL_KDD, y_train_NSL_KDD, y_test_NSL_KDD = train_test_split(data[:,1:], one_hot_labels,stratify=one_hot_labels, test_size=0.2, random_state=12) # 固定了随机数种子导致分割的数据集是重复的\n",
        "\n",
        "y_train_NSL_KDD = np.argmax(y_train_NSL_KDD, axis=1)\n",
        "y_test_NSL_KDD = np.argmax(y_test_NSL_KDD, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI-tS1crojGC"
      },
      "source": [
        "## 最近邻分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_mpj5QOi_Bm",
        "outputId": "0e5b6d4c-f86e-4e80-fedb-ed993297e045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.6629794152658015\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7119    0.8139    0.7595     15411\n",
            "           1     0.6120    0.5338    0.5702     10740\n",
            "           2     0.0028    0.7500    0.0057         4\n",
            "           3     0.9996    0.5781    0.7326      4736\n",
            "           4     0.7355    0.4860    0.5852      2815\n",
            "           5     0.6121    0.6078    0.6100       714\n",
            "           6     0.0146    0.6957    0.0287        23\n",
            "\n",
            "    accuracy                         0.6630     34443\n",
            "   macro avg     0.5270    0.6379    0.4703     34443\n",
            "weighted avg     0.7196    0.6630    0.6789     34443\n",
            "\n",
            "准确率: 0.6630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_nearest_centroid.py:244: UserWarning: self.within_class_std_dev_ has at least 1 zero standard deviation.Inputs within the same classes for at least 1 feature are identical.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练分类器\n",
        "centroid_classifier = NearestCentroid()\n",
        "centroid_classifier.fit(x_train_NSL_KDD, y_train_NSL_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = centroid_classifier.predict(x_test_NSL_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_NSL_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_NSL_KDD, y_pred, digits=4))\n",
        "\n",
        "accuracy = accuracy_score(y_test_NSL_KDD, y_pred)\n",
        "print(f\"准确率: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMAiUmH-ooUj"
      },
      "source": [
        "## Logistic 回归"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR2IcKBuopiq",
        "outputId": "d3874c4f-83a7-4289-d9ab-cc4e45c6e164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.8716139709084575\n",
            "准确率: 0.8716\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8667    0.9312    0.8978     15411\n",
            "           1     0.9254    0.8986    0.9118     10740\n",
            "           2     0.0000    0.0000    0.0000         4\n",
            "           3     0.9020    0.8587    0.8798      4736\n",
            "           4     0.7207    0.6259    0.6700      2815\n",
            "           5     0.3780    0.2647    0.3114       714\n",
            "           6     1.0000    0.0435    0.0833        23\n",
            "\n",
            "    accuracy                         0.8716     34443\n",
            "   macro avg     0.6847    0.5175    0.5363     34443\n",
            "weighted avg     0.8678    0.8716    0.8683     34443\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练逻辑回归模型\n",
        "lr_classifier = LogisticRegression(max_iter=200)\n",
        "lr_classifier.fit(x_train_NSL_KDD, y_train_NSL_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = lr_classifier.predict(x_test_NSL_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_NSL_KDD, y_pred))\n",
        "\n",
        "\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_NSL_KDD, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6_ykZ_TopH0"
      },
      "source": [
        "## 朴素贝叶斯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLFOO00PpDR8",
        "outputId": "62742ada-e7b8-402a-d570-fe3129c59ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.7062973608570682\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.5404    0.7016     15411\n",
            "           1     0.7749    0.7555    0.7651     10740\n",
            "           2     0.1176    1.0000    0.2105         4\n",
            "           3     1.0000    0.9937    0.9968      4736\n",
            "           4     0.3315    0.9861    0.4962      2815\n",
            "           5     0.2279    0.5336    0.3194       714\n",
            "           6     0.0210    0.7826    0.0409        23\n",
            "\n",
            "    accuracy                         0.7063     34443\n",
            "   macro avg     0.4961    0.7988    0.5044     34443\n",
            "weighted avg     0.8584    0.7063    0.7368     34443\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练朴素贝叶斯模型\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(x_train_NSL_KDD, y_train_NSL_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = nb_classifier.predict(x_test_NSL_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_NSL_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_NSL_KDD, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI6um3jHpGOo"
      },
      "source": [
        "## 决策树"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJIY3yPZpFs6",
        "outputId": "77b28c72-b2af-4de5-ea02-b38821a87823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.7288563713962198\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8327    1.0000    0.9087     15411\n",
            "           1     0.6082    0.9025    0.7267     10740\n",
            "           2     0.0000    0.0000    0.0000         4\n",
            "           3     0.0000    0.0000    0.0000      4736\n",
            "           4     0.0000    0.0000    0.0000      2815\n",
            "           5     0.0000    0.0000    0.0000       714\n",
            "           6     0.0000    0.0000    0.0000        23\n",
            "\n",
            "    accuracy                         0.7289     34443\n",
            "   macro avg     0.2059    0.2718    0.2336     34443\n",
            "weighted avg     0.5622    0.7289    0.6332     34443\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tree_classifier = DecisionTreeClassifier(max_depth=1)\n",
        "tree_classifier.fit(x_train_NSL_KDD, y_train_NSL_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = tree_classifier.predict(x_test_NSL_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_NSL_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_NSL_KDD, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUiQT2PArewS"
      },
      "source": [
        "## 消融实验"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb2gv5DErjPI",
        "outputId": "c0d86aef-398f-46e8-bbca-07eae5c93ea8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.3072  |  0:00:05s\n",
            "epoch 1  | loss: 0.04735 |  0:00:10s\n",
            "epoch 2  | loss: 0.03519 |  0:00:15s\n",
            "epoch 3  | loss: 0.02322 |  0:00:21s\n",
            "epoch 4  | loss: 0.02021 |  0:00:27s\n",
            "epoch 5  | loss: 0.01239 |  0:00:34s\n",
            "epoch 6  | loss: 0.01727 |  0:00:40s\n",
            "epoch 7  | loss: 0.01762 |  0:00:45s\n",
            "epoch 8  | loss: 0.01343 |  0:00:51s\n",
            "epoch 9  | loss: 0.00971 |  0:00:56s\n",
            "epoch 10 | loss: 0.00864 |  0:01:02s\n",
            "epoch 11 | loss: 0.00737 |  0:01:07s\n",
            "epoch 12 | loss: 0.0065  |  0:01:13s\n",
            "epoch 13 | loss: 0.00649 |  0:01:19s\n",
            "epoch 14 | loss: 0.00601 |  0:01:24s\n",
            "epoch 15 | loss: 0.00559 |  0:01:29s\n",
            "epoch 16 | loss: 0.00777 |  0:01:35s\n",
            "epoch 17 | loss: 0.00811 |  0:01:40s\n",
            "epoch 18 | loss: 0.00677 |  0:01:45s\n",
            "epoch 19 | loss: 0.00812 |  0:01:50s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 定义TabNet模型\n",
        "tabnet_params = {\n",
        "    'n_d': 8,\n",
        "    'n_a': 8,\n",
        "    'n_steps': 3,\n",
        "    'gamma': 1.5,\n",
        "    'n_independent': 2,\n",
        "    'n_shared': 2,\n",
        "    'momentum': 0.02,\n",
        "    'lambda_sparse': 1e-3,\n",
        "    'optimizer_fn': torch.optim.Adam,\n",
        "    'optimizer_params': dict(lr=2e-2),\n",
        "    'mask_type': 'sparsemax',\n",
        "    'scheduler_params': {\"gamma\": 0.95, 'step_size': 10},\n",
        "    'scheduler_fn': torch.optim.lr_scheduler.StepLR\n",
        "}\n",
        "\n",
        "X_train = torch.tensor(x_train_NSL_KDD, dtype=torch.float32)\n",
        "X_test = torch.tensor(x_test_NSL_KDD, dtype=torch.float32)\n",
        "\n",
        "if y_train_NSL_KDD.ndim == 2:\n",
        "    y_train = torch.tensor(np.argmax(y_train_NSL_KDD, axis=1), dtype=torch.long)\n",
        "    y_test = torch.tensor(np.argmax(y_test_NSL_KDD, axis=1), dtype=torch.long)\n",
        "else:\n",
        "    y_train = torch.tensor(y_train_NSL_KDD, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test_NSL_KDD, dtype=torch.long)\n",
        "\n",
        "# 初始化TabNet分类器\n",
        "clf = TabNetClassifier(**tabnet_params)\n",
        "\n",
        "# 训练TabNet模型\n",
        "clf.fit(\n",
        "    X_train=X_train.numpy(),\n",
        "    y_train=y_train.numpy(),\n",
        "    max_epochs=20,\n",
        "    patience=20,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPt34g4Vrof2",
        "outputId": "12daf930-bd70-4e0d-a08c-3c487aa47fe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.9981\n",
            "Accuracy: 0.9981\n",
            "Detection Rate: 0.7675\n",
            "Confusion Matrix:\n",
            "[[15411     0     0     0     0     0     0]\n",
            " [    0 10711     0     0     3    26     0]\n",
            " [    0     0     0     4     0     0     0]\n",
            " [    0     0     0  4736     0     0     0]\n",
            " [    0    10     0     0  2805     0     0]\n",
            " [    0     3     0     0     5   705     1]\n",
            " [    0     4     0     0     5     5     9]]\n",
            "False Alarm Rate: 0.0003\n",
            "False Negative Rate: 0.2325\n"
          ]
        }
      ],
      "source": [
        "# 评估模型\n",
        "y_pred = clf.predict(x_test_NSL_KDD)\n",
        "accuracy = accuracy_score(y_test_NSL_KDD, y_pred)\n",
        "print(f'Final Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "y_test_original = le.inverse_transform(y_test)\n",
        "y_pred_original = le.inverse_transform(y_pred)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score, confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# 预测\n",
        "y_pred = clf.predict(x_test_NSL_KDD)\n",
        "\n",
        "accuracy = accuracy_score(y_test_NSL_KDD, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "detection_rate = recall_score(y_test_NSL_KDD, y_pred, average='macro')  # 使用 'macro' 或 'weighted' 平均\n",
        "print(f'Detection Rate: {detection_rate:.4f}')\n",
        "conf_matrix = confusion_matrix(y_test_NSL_KDD, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "false_alarm_rate = []\n",
        "false_negative_rate = []\n",
        "\n",
        "for i in range(len(conf_matrix)):\n",
        "    tp = conf_matrix[i, i]\n",
        "    fp = conf_matrix[:, i].sum() - tp\n",
        "    fn = conf_matrix[i, :].sum() - tp\n",
        "    tn = conf_matrix.sum() - (tp + fp + fn)\n",
        "\n",
        "    if fp + tn > 0:\n",
        "        false_alarm_rate.append(fp / (fp + tn))\n",
        "    if fn + tp > 0:\n",
        "        false_negative_rate.append(fn / (fn + tp))\n",
        "\n",
        "avg_false_alarm_rate = sum(false_alarm_rate) / len(false_alarm_rate)\n",
        "avg_false_negative_rate = sum(false_negative_rate) / len(false_negative_rate)\n",
        "\n",
        "print(f'False Alarm Rate: {avg_false_alarm_rate:.4f}')\n",
        "print(f'False Negative Rate: {avg_false_negative_rate:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKwrE_mPGQ9"
      },
      "source": [
        "# 对比试验(KDD-UEBA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FhqOe70S-3_"
      },
      "source": [
        "## 数据预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgFxL7vdS-3_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "NSL_KDD = pd.read_csv('/content/drive/MyDrive/kddcup99(raw).csv',encoding='gbk')\n",
        "NSL_KDD.columns = NSL_KDD.columns.str.strip()\n",
        "NSL_KDD.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "NSL_KDD.dropna(inplace=True)\n",
        "def map_label_to_category(label):\n",
        "    probe_attacks = ['satan', 'ipsweep', 'nmap', 'portsweep', 'mscan', 'saint']\n",
        "    dos_attacks = ['neptune', 'smurf', 'back', 'teardrop', 'pod', 'land', 'apache2', 'processtable', 'mailbomb', 'snmpgetattack', 'httptunnel', 'worm', 'sqlattack', 'udpstorm']\n",
        "    u2r_attacks = ['buffer_overflow', 'loadmodule', 'rootkit', 'perl', 'xterm', 'ps']\n",
        "    r2l_attacks = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster', 'warezclient', 'spy', 'named', 'sendmail', 'snmpguess', 'xlock', 'xsnoop']\n",
        "\n",
        "    if label in probe_attacks:\n",
        "        return 'Probe'\n",
        "    elif label in dos_attacks:\n",
        "        return 'DoS'\n",
        "    elif label in u2r_attacks:\n",
        "        return 'U2R'\n",
        "    elif label in r2l_attacks:\n",
        "        return 'R2L'\n",
        "    elif label == 'normal':\n",
        "        return 'BENIGN'\n",
        "\n",
        "\n",
        "NSL_KDD['label'] = NSL_KDD['label'].apply(map_label_to_category)\n",
        "category_counts = NSL_KDD['label'].value_counts()\n",
        "NSL_KDD['label'].value_counts()\n",
        "\n",
        "values_list = ['BENIGN','Probe', 'DoS', 'U2R','R2L']\n",
        "filtered_df = NSL_KDD[NSL_KDD['label'].isin(values_list)]\n",
        "\n",
        "NSL_KDD['label'] = NSL_KDD['label'].apply(map_label_to_category)\n",
        "\n",
        "pd_train_v1 = pd.read_csv('/content/drive/MyDrive/CVUEBA.csv',encoding='gbk')\n",
        "pd_train_v1 = pd_train_v1.rename(columns={'attack': 'label'})\n",
        "\n",
        "\n",
        "def get_label(Label):\n",
        "    if Label == 0:\n",
        "        return 'Normal'\n",
        "    else:\n",
        "        return 'Malicious'\n",
        "\n",
        "pd_train_v1['label'] = pd_train_v1['label'].apply(get_label)\n",
        "# pd_train_v1.drop(columns=['day'],inplace=True)\n",
        "\n",
        "all_columns = set(filtered_df.columns) | set(pd_train_v1.columns)\n",
        "filtered_df_v1 = filtered_df.reindex(columns=all_columns)\n",
        "df_train_v2 = pd_train_v1.reindex(columns=all_columns)\n",
        "pd_final = pd.concat([filtered_df_v1, df_train_v2], ignore_index=True)\n",
        "\n",
        "nb_class = len(list(pd_final['label'].value_counts()))\n",
        "pd_mini = pd_final.sample(n=len(pd_final))\n",
        "\n",
        "object_cols = pd_mini.select_dtypes(include=['object']).columns.tolist()\n",
        "object_cols.remove('label')\n",
        "\n",
        "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value=-99))\n",
        "cat_ohe_step = ('ohe', OrdinalEncoder()) #离散数据整数化处理\n",
        "cat_steps = [cat_si_step, cat_ohe_step]\n",
        "\n",
        "cat_pipe = Pipeline(cat_steps)\n",
        "cat_cols = object_cols\n",
        "cat_transformers = [('cat', cat_pipe, cat_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=cat_transformers)\n",
        "\n",
        "\n",
        "for column_name in cat_cols:\n",
        "  pd_mini[column_name] = pd_mini[column_name].astype('str')\n",
        "X_kdd_transformed = ct.fit_transform(pd_mini)\n",
        "non_object_cols = pd_mini.select_dtypes(exclude=['object']).columns.tolist()\n",
        "num_cols = non_object_cols\n",
        "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
        "num_ss_step = ('ss', StandardScaler()) #均值-标准差归一化\n",
        "num_steps = [num_si_step, num_ss_step]\n",
        "num_pipe = Pipeline(num_steps)\n",
        "num_transformers = [('num', num_pipe, num_cols)]\n",
        "ct = ColumnTransformer(transformers=num_transformers)\n",
        "X_num_transformed = ct.fit_transform(pd_mini)\n",
        "\n",
        "transformers = [('cat', cat_pipe, cat_cols),\n",
        "                ('num', num_pipe, num_cols)]\n",
        "\n",
        "ct = ColumnTransformer(transformers=transformers)\n",
        "datax = pd_mini.drop(columns=['label'])\n",
        "x_all = ct.fit_transform(datax)\n",
        "le = LabelEncoder()\n",
        "y_all = le.fit_transform(pd_mini['label'])\n",
        "y_all = y_all.reshape(-1,1)\n",
        "data = np.concatenate((y_all.reshape(-1,1),x_all),axis=1)\n",
        "c = []\n",
        "\n",
        "#转化为独热编码\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_samples = len(labels)\n",
        "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
        "    for i in range(num_samples):\n",
        "        one_hot_labels[i, labels[i]] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "num_classes = 7\n",
        "one_hot_labels = one_hot_encode(data[:,0].astype(int), num_classes)\n",
        "\n",
        "x_train_KDD, x_test_KDD, y_train_KDD, y_test_KDD = train_test_split(data[:,1:], one_hot_labels,stratify=one_hot_labels, test_size=0.2, random_state=12) # 固定了随机数种子导致分割的数据集是重复的\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID_xqb_yqoGU",
        "outputId": "9067bf6e-976c-44a2-c411-1f61d6d95e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85jY9xNjq5I1"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JWTWQOOqluq"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(x_train_KDD, dtype=torch.float32)\n",
        "X_test = torch.tensor(x_test_KDD, dtype=torch.float32)\n",
        "\n",
        "if y_train_KDD.ndim == 2:\n",
        "    y_train = torch.tensor(np.argmax(y_train_KDD, axis=1), dtype=torch.long)\n",
        "    y_test = torch.tensor(np.argmax(y_test_KDD, axis=1), dtype=torch.long)\n",
        "else:\n",
        "    y_train = torch.tensor(y_train_KDD, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test_KDD, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC9Fglh1S-4A"
      },
      "outputs": [],
      "source": [
        "y_train_KDD = np.argmax(y_train_KDD, axis=1)\n",
        "y_test_KDD = np.argmax(y_test_KDD, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b09JIyBQqFHo"
      },
      "source": [
        "## 最近邻分类（Nearest Centroid Classifier）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-f2b5MXqFHp",
        "outputId": "2eba6bed-504d-4f1b-9754-0272ad589eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.6454840454299622\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8239    0.2600    0.3952     19456\n",
            "           1     0.8524    0.7322    0.7878     78292\n",
            "           2     0.6667    0.6667    0.6667         3\n",
            "           3     0.9997    0.8055    0.8922      4736\n",
            "           4     0.0214    0.6800    0.0414       822\n",
            "           5     0.4776    0.2844    0.3565       225\n",
            "           6     0.3000    0.9000    0.4500        10\n",
            "\n",
            "    accuracy                         0.6455    103544\n",
            "   macro avg     0.5917    0.6184    0.5128    103544\n",
            "weighted avg     0.8463    0.6455    0.7119    103544\n",
            "\n",
            "准确率: 0.6455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_nearest_centroid.py:244: UserWarning: self.within_class_std_dev_ has at least 1 zero standard deviation.Inputs within the same classes for at least 1 feature are identical.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练最近邻分类器\n",
        "centroid_classifier = NearestCentroid()\n",
        "centroid_classifier.fit(x_train_KDD, y_train_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = centroid_classifier.predict(x_test_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_KDD, y_pred, digits=4))\n",
        "\n",
        "accuracy = accuracy_score(y_test_KDD, y_pred)\n",
        "print(f\"准确率: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH1UlyVOqI50"
      },
      "source": [
        "## Logistic 回归（Logistic Regression）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kSGCx1IqI51",
        "outputId": "bbae273f-4fe1-4e74-80be-b0b25f5f5215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.9860252646218033\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9529    0.9814    0.9669     19456\n",
            "           1     0.9958    0.9940    0.9949     78292\n",
            "           2     0.0000    0.0000    0.0000         3\n",
            "           3     0.9748    0.9799    0.9774      4736\n",
            "           4     0.9312    0.5925    0.7242       822\n",
            "           5     0.7260    0.2356    0.3557       225\n",
            "           6     0.0000    0.0000    0.0000        10\n",
            "\n",
            "    accuracy                         0.9860    103544\n",
            "   macro avg     0.6544    0.5405    0.5742    103544\n",
            "weighted avg     0.9856    0.9860    0.9852    103544\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练逻辑回归模型\n",
        "lr_classifier = LogisticRegression(max_iter=200)\n",
        "lr_classifier.fit(x_train_KDD, y_train_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = lr_classifier.predict(x_test_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_KDD, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KZ4YlsjqLw0"
      },
      "source": [
        "## 朴素贝叶斯（Naive Bayes）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX415hqEsi6z",
        "outputId": "1b72bc5b-8d2e-4b17-e45a-1ef0590b1e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.8866472224368385\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9659    0.6443    0.7730     19456\n",
            "           1     0.9769    0.9407    0.9584     78292\n",
            "           2     0.1500    1.0000    0.2609         3\n",
            "           3     1.0000    0.9964    0.9982      4736\n",
            "           4     0.0924    0.9830    0.1689       822\n",
            "           5     0.3772    0.3822    0.3797       225\n",
            "           6     0.0062    0.9000    0.0122        10\n",
            "\n",
            "    accuracy                         0.8866    103544\n",
            "   macro avg     0.5098    0.8352    0.5073    103544\n",
            "weighted avg     0.9674    0.8866    0.9178    103544\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练朴素贝叶斯模型\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(x_train_KDD, y_train_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = nb_classifier.predict(x_test_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_KDD, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fzgO4W9qOFT"
      },
      "source": [
        "## 决策树（DT）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWAjsZEKqOFU",
        "outputId": "e3602d0d-9b9f-4fed-9ca0-d1789e78c65d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.9321158154987251\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9178    0.9863    0.9508     19456\n",
            "           1     0.9357    0.9876    0.9610     78292\n",
            "           2     0.0000    0.0000    0.0000         3\n",
            "           3     0.0000    0.0000    0.0000      4736\n",
            "           4     0.0000    0.0000    0.0000       822\n",
            "           5     0.0000    0.0000    0.0000       225\n",
            "           6     0.0000    0.0000    0.0000        10\n",
            "\n",
            "    accuracy                         0.9321    103544\n",
            "   macro avg     0.2648    0.2820    0.2731    103544\n",
            "weighted avg     0.8800    0.9321    0.9053    103544\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tree_classifier = DecisionTreeClassifier(max_depth=1)\n",
        "tree_classifier.fit(x_train_KDD, y_train_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = tree_classifier.predict(x_test_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_KDD, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6mCV4xBqLw1",
        "outputId": "e16e1ee9-34aa-43d3-f68a-fcdaee1954b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "准确率: 0.8866472224368385\n",
            "分类报告:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9659    0.6443    0.7730     19456\n",
            "           1     0.9769    0.9407    0.9584     78292\n",
            "           2     0.1500    1.0000    0.2609         3\n",
            "           3     1.0000    0.9964    0.9982      4736\n",
            "           4     0.0924    0.9830    0.1689       822\n",
            "           5     0.3772    0.3822    0.3797       225\n",
            "           6     0.0062    0.9000    0.0122        10\n",
            "\n",
            "    accuracy                         0.8866    103544\n",
            "   macro avg     0.5098    0.8352    0.5073    103544\n",
            "weighted avg     0.9674    0.8866    0.9178    103544\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 训练朴素贝叶斯模型\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(x_train_KDD, y_train_KDD)\n",
        "\n",
        "# 预测\n",
        "y_pred = nb_classifier.predict(x_test_KDD)\n",
        "\n",
        "# 评估\n",
        "print(\"准确率:\", accuracy_score(y_test_KDD, y_pred))\n",
        "print(\"分类报告:\")\n",
        "print(classification_report(y_test_KDD, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UM-_ub0r_T2"
      },
      "source": [
        "## 消融实验"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vgJtkMMr_T3",
        "outputId": "f56f886c-bf92-4a42-ecbf-97c5bbecbb34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.09425 |  0:00:15s\n",
            "epoch 1  | loss: 0.01664 |  0:00:31s\n",
            "epoch 2  | loss: 0.01301 |  0:00:46s\n",
            "epoch 3  | loss: 0.00962 |  0:01:02s\n",
            "epoch 4  | loss: 0.00912 |  0:01:17s\n",
            "epoch 5  | loss: 0.00793 |  0:01:33s\n",
            "epoch 6  | loss: 0.0074  |  0:01:48s\n",
            "epoch 7  | loss: 0.00721 |  0:02:04s\n",
            "epoch 8  | loss: 0.00817 |  0:02:20s\n",
            "epoch 9  | loss: 0.01094 |  0:02:36s\n",
            "epoch 10 | loss: 0.00922 |  0:02:51s\n",
            "epoch 11 | loss: 0.0078  |  0:03:07s\n",
            "epoch 12 | loss: 0.00949 |  0:03:22s\n",
            "epoch 13 | loss: 0.00725 |  0:03:38s\n",
            "epoch 14 | loss: 0.0073  |  0:03:53s\n",
            "epoch 15 | loss: 0.00676 |  0:04:09s\n",
            "epoch 16 | loss: 0.00674 |  0:04:24s\n",
            "epoch 17 | loss: 0.00632 |  0:04:40s\n",
            "epoch 18 | loss: 0.00563 |  0:04:56s\n",
            "epoch 19 | loss: 0.00938 |  0:05:11s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 定义TabNet模型\n",
        "tabnet_params = {\n",
        "    'n_d': 8,\n",
        "    'n_a': 8,\n",
        "    'n_steps': 3,\n",
        "    'gamma': 1.5,\n",
        "    'n_independent': 2,\n",
        "    'n_shared': 2,\n",
        "    'momentum': 0.02,\n",
        "    'lambda_sparse': 1e-3,\n",
        "    'optimizer_fn': torch.optim.Adam,\n",
        "    'optimizer_params': dict(lr=2e-2),\n",
        "    'mask_type': 'sparsemax',\n",
        "    'scheduler_params': {\"gamma\": 0.95, 'step_size': 10},\n",
        "    'scheduler_fn': torch.optim.lr_scheduler.StepLR\n",
        "}\n",
        "\n",
        "X_train = torch.tensor(x_train_KDD, dtype=torch.float32)\n",
        "X_test = torch.tensor(x_test_KDD, dtype=torch.float32)\n",
        "if y_train_KDD.ndim == 2:\n",
        "    y_train = torch.tensor(np.argmax(y_train_KDD, axis=1), dtype=torch.long)\n",
        "    y_test = torch.tensor(np.argmax(y_test_KDD, axis=1), dtype=torch.long)\n",
        "else:\n",
        "    y_train = torch.tensor(y_train_KDD, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test_KDD, dtype=torch.long)\n",
        "\n",
        "# 初始化TabNet分类器\n",
        "clf = TabNetClassifier(**tabnet_params)\n",
        "\n",
        "# 训练TabNet模型\n",
        "clf.fit(\n",
        "    X_train=X_train.numpy(),\n",
        "    y_train=y_train.numpy(),\n",
        "    max_epochs=20,\n",
        "    patience=20,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCWmgmuJr_T4",
        "outputId": "34de754b-28c1-46e6-c60e-1ba8d50b99df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.9946\n",
            "Accuracy: 0.9946\n",
            "Detection Rate: 0.8313\n",
            "Confusion Matrix:\n",
            "[[19401    14     0     0     7    34     0]\n",
            " [  433 77858     0     1     0     0     0]\n",
            " [    0     0     3     0     0     0     0]\n",
            " [    0     0     4  4732     0     0     0]\n",
            " [   14     2     0     0   805     1     0]\n",
            " [   29     3     0     0     2   191     0]\n",
            " [    1     0     0     0     0     9     0]]\n",
            "False Alarm Rate: 0.0010\n",
            "False Negative Rate: 0.1687\n"
          ]
        }
      ],
      "source": [
        "# 评估模型\n",
        "y_pred = clf.predict(x_test_KDD)\n",
        "accuracy = accuracy_score(y_test_KDD, y_pred)\n",
        "print(f'Final Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 将预测和真实标签直接映射回原始类别标签\n",
        "y_test_original = le.inverse_transform(y_test)\n",
        "y_pred_original = le.inverse_transform(y_pred)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score, confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# 预测\n",
        "y_pred = clf.predict(x_test_KDD)\n",
        "\n",
        "accuracy = accuracy_score(y_test_KDD, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "detection_rate = recall_score(y_test_KDD, y_pred, average='macro')  # 使用 'macro' 或 'weighted' 平均\n",
        "print(f'Detection Rate: {detection_rate:.4f}')\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_KDD, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "false_alarm_rate = []\n",
        "false_negative_rate = []\n",
        "\n",
        "for i in range(len(conf_matrix)):\n",
        "    tp = conf_matrix[i, i]\n",
        "    fp = conf_matrix[:, i].sum() - tp\n",
        "    fn = conf_matrix[i, :].sum() - tp\n",
        "    tn = conf_matrix.sum() - (tp + fp + fn)\n",
        "\n",
        "    if fp + tn > 0:\n",
        "        false_alarm_rate.append(fp / (fp + tn))\n",
        "    if fn + tp > 0:\n",
        "        false_negative_rate.append(fn / (fn + tp))\n",
        "\n",
        "avg_false_alarm_rate = sum(false_alarm_rate) / len(false_alarm_rate)\n",
        "avg_false_negative_rate = sum(false_negative_rate) / len(false_negative_rate)\n",
        "\n",
        "print(f'False Alarm Rate: {avg_false_alarm_rate:.4f}')\n",
        "print(f'False Negative Rate: {avg_false_negative_rate:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hkKa7vVDR_Yn",
        "ACmyS7kgSBe6",
        "upKwrE_mPGQ9"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
